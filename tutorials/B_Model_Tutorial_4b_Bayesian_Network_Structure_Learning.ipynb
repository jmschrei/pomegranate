{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network Structure Learning\n",
    "\n",
    "author: Jacob Schreiber <br>\n",
    "contact:\n",
    "jmschreiber91@gmail.com\n",
    "    \n",
    "Learning the structure of Bayesian networks can be\n",
    "complicated for two main reasons: (1) difficulties in inferring causality and\n",
    "(2) the super-exponential number of directed edges that could exist in a\n",
    "dataset. The first issue presents itself when the structure lerning algorithm\n",
    "considers only correlation or another measure of co-occurrence to determine if\n",
    "an edge should exist. The first point presents challenges which deserve a far\n",
    "more in depth treatment unrelated to implementations in pomegranate, so instead\n",
    "this tutorial will focus on how pomegranate implements fast Bayesian network\n",
    "structure learning. It will also cover a new concept called the \"constraint\n",
    "graph\" which can be used to massively speed up structure search while also\n",
    "making causality assignment a bit more reasonable.\n",
    "\n",
    "### Introduction to Bayesian\n",
    "Network Structure Learning\n",
    "\n",
    "Most methods for Bayesian network structure learning\n",
    "(BNSL) can be put into one of the following three categories: \n",
    "\n",
    "(1) Search and\n",
    "Score: The most intuitive method is that of 'search and score,' where one\n",
    "searches over the space of all possible directed acyclic graphs (DAGs) and\n",
    "identifies the one that minimizes some objective function. Typical objective\n",
    "functions attempt to balance the log probability of the data given the model\n",
    "(the likelihood) with the complexity of the model to encourage sparser models. A\n",
    "naive implementation of this search is super-exponential in time with the number\n",
    "of variables, and becomes infeasible when considering even less than a dozen\n",
    "variables. However, dynamic programming can efficiently remove the many repeated\n",
    "calculations and reduce this to be simply exponential in time. This allows exact\n",
    "BNSL to scale to ~25-30 variables. In addition, the A\\* algorithm can be used to\n",
    "smartly search the space and reduce computational time even further by not even\n",
    "considering all possibile networks.\n",
    "\n",
    "(2) Constraint learning: These methods\n",
    "typically involve calculating some measure of correlation or co-occurrence to\n",
    "identify an undirected backbone of edges that could exist, and then prune these\n",
    "edges systematically until a DAG is reached. A common method is to iterate over\n",
    "all triplets of variables to identify conditional independencies that specify\n",
    "both presence and direction of the edges. This algorithm is asymptotically\n",
    "faster (quadratic in time) than search-and-score, but it does not have a simple\n",
    "probabilistic interpretation.\n",
    "\n",
    "(3) Approximate algorithms: In many real world\n",
    "examples, one wishes to merge the interpretability of the search and score\n",
    "method with the attractiveness of the task finishing before the universe ends.\n",
    "To this end, several heuristics have been developed with different properties to\n",
    "yield good structures in a reasonable amount of time. These methods include the\n",
    "Chow-Liu tree building algorithm, the hill-climbing algorithm, and optimal\n",
    "reinsertion, though there are others.\n",
    "\n",
    "pomegranate currently implements a\n",
    "search-and-score method based on the minimum description length score which\n",
    "utilizes the dynamic programming and A\\* algorithm (DP/A\\*), a greedy algorithm\n",
    "based off of DP/A\\*, and the Chow-Liu tree building algorithm, though there are\n",
    "plans to soon add other algorithms.\n",
    "\n",
    "### Structure Learning in pomegranate\n",
    "####\n",
    "Exact Learning\n",
    "\n",
    "Structure learning in pomegranate is done using the\n",
    "`from_samples` method. All you pass in is the samples, their associated weights\n",
    "(if not uniform), and the algorithm which you'd like to use, and it will learn\n",
    "the network for you using the dynamic programming implementation. Lets see a\n",
    "quick synthetic example to make sure that appropriate connections are found.\n",
    "Lets add connections between variables 1, 3, 6, and variables 0 and 2, and\n",
    "variables 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext memory_profiler\n",
    "from pomegranate import BayesianNetwork\n",
    "import seaborn, time\n",
    "seaborn.set_style('whitegrid')\n",
    "\n",
    "X = numpy.random.randint(2, size=(2000, 7))\n",
    "X[:,3] = X[:,1]\n",
    "X[:,6] = X[:,1]\n",
    "\n",
    "X[:,0] = X[:,2]\n",
    "\n",
    "X[:,4] = X[:,5]\n",
    "\n",
    "model = BayesianNetwork.from_samples(X, algorithm='exact')\n",
    "print( model.structure)\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure attribute returns a tuple of tuples, where each inner tuple\n",
    "corresponds to that node in the graph (and the column of data learned on). The\n",
    "numbers in that inner tuple correspond to the parents of that node. The results\n",
    "from this structure are that node 3 has node 1 as a parent, that node 2 has node\n",
    "0 as a parent, and so forth. It seems to faithfully recapture the underlying\n",
    "dependencies in the data.\n",
    "\n",
    "Now, two algorithms for performing search-and-score\n",
    "were mentioned, the traditional shortest path algorithm and the A\\* algorithm.\n",
    "These both work by essentially turning the Bayesian network structure learning\n",
    "problem into a shortest path problem over an 'order graph.' This order graph is\n",
    "a lattice made up of layers of variable sets from the BNSL problem, with the\n",
    "root node having no variables, the leaf node having all variables, and layer `i`\n",
    "in the lattice having all subsets of variables of size `i`. Each path from the\n",
    "root to the leaf represents a certain topological sort of the variables, with\n",
    "the shortest path corresponding to the optimal topological sort and Bayesian\n",
    "network. Details can be found <a\n",
    "href=\"http://url.cs.qc.cuny.edu/publications/Yuan11learning.pdf\">here</a>. The\n",
    "traditional shortest path algorithm calculates the values of all edges in the\n",
    "order lattice before finding the shortest path, while the A\\* algorithm searches\n",
    "only a subset of the order lattice and begins searching immediately. Both\n",
    "methods yield optimal Bayesian networks.\n",
    "\n",
    "A major problem that arises in the\n",
    "traditional shortest path algorithm is that the size of the order graph grows\n",
    "exponentially with the number of variables, and can make tasks infeasible that\n",
    "have otherwise-reasonable computational times. While the A\\* algorithm is faster\n",
    "computationally, another advantage is that it uses a much smaller amount of\n",
    "memory since it doesn't explore the full order graph, and so can be applied to\n",
    "larger problems.\n",
    "\n",
    "In order to see the differences between these algorithms in\n",
    "practice, let's turn to the task of learning a Bayesian network over the digits\n",
    "dataset. The digits dataset is comprised of over a thousand 8x8 pictures of\n",
    "handwritten digits. We binarize the values into 'on' or 'off' for simplicity,\n",
    "and try to learn dependencies between the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(10, True)\n",
    "X = X > numpy.mean(X)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(X[0].reshape(8, 8), interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.subplot(132)\n",
    "plt.imshow(X[1].reshape(8, 8), interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.subplot(133)\n",
    "plt.imshow(X[2].reshape(8, 8), interpolation='nearest')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,:18]\n",
    "    \n",
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X, algorithm='exact-dp') # << BNSL done here!\n",
    "t1 = time.time() - tic\n",
    "p1 = model.log_probability(X).sum()\n",
    "\n",
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X, algorithm='exact')\n",
    "t2 = time.time() - tic\n",
    "p2 = model.log_probability(X).sum()\n",
    "\n",
    "\n",
    "print( \"Shortest Path\")\n",
    "print( \"Time (s): \", t1)\n",
    "print( \"P(D|M): \", p1)\n",
    "%memit BayesianNetwork.from_samples(X, algorithm='exact-dp')\n",
    "print( \"A* Search\")\n",
    "print( \"Time (s): \", t2)\n",
    "print( \"P(D|M): \", p2)\n",
    "%memit BayesianNetwork.from_samples(X, algorithm='exact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that the A\\* algorithm is both computationally faster and\n",
    "requires far less memory than the traditional algorithm, making it a better\n",
    "default for the 'exact' algorithm. The amount of memory used by the BNSL process\n",
    "is under 'increment', not 'peak memory', as 'peak memory' returns the total\n",
    "memory used by everything, while increment shows the difference in peak memory\n",
    "before and after the function has run. \n",
    "\n",
    "#### Approximate Learning: Greedy\n",
    "Search (pomegranate default)\n",
    "\n",
    "A natural heuristic when a non-greedy algorithm is\n",
    "too slow is to consider the greedy version. This simple implementation\n",
    "iteratively finds the best variable to add to the growing topological sort,\n",
    "allowing the new variable to draw only from variables already in the topological\n",
    "sort. This is the default in pomegranate because it has a nice balance between\n",
    "producing good (often optimal) graphs and having a small computational cost and\n",
    "memory footprint. However, there is no guarantee that this produces the globally\n",
    "optimal graph.\n",
    "\n",
    "Let's see how it performs on the same dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X) # << Default BNSL setting\n",
    "t = time.time() - tic\n",
    "p = model.log_probability(X).sum()\n",
    "\n",
    "print( \"Greedy\")\n",
    "print( \"Time (s): \", t)\n",
    "print( \"P(D|M): \", p)\n",
    "%memit BayesianNetwork.from_samples(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Learning: Chow-Liu Trees\n",
    "\n",
    "However, there are even cases where\n",
    "the greedy heuristic is too slow, for example hundreds of variables. One of the\n",
    "first heuristics for BNSL is that of Chow-Liu trees, which learns the optimal\n",
    "tree from data. Essentially it calculates the mutual information between all\n",
    "pairs of variables and then finds the maximum spanning tree. A root node has to\n",
    "be input to turn the undirected edges based on mutual information into directed\n",
    "edges for the Bayesian network. The algorithm is is $O(d^{2})$ and practically\n",
    "is extremely fast and memory efficient, though it produces structures with a\n",
    "worse $P(D|M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X, algorithm='chow-liu') # << Default BNSL setting\n",
    "t = time.time() - tic\n",
    "p = model.log_probability(X).sum()\n",
    "\n",
    "print( \"Chow-Liu\")\n",
    "print( \"Time (s): \", t)\n",
    "print( \"P(D|M): \", p)\n",
    "%memit BayesianNetwork.from_samples(X, algorithm='chow-liu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "We can then compare the algorithms directly to each other on\n",
    "the digits dataset as we expand the number of pixels to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = load_digits(10, True)\n",
    "X = X > numpy.mean(X)\n",
    "\n",
    "t1, t2, t3, t4 = [], [], [], []\n",
    "p1, p2, p3, p4 = [], [], [], []\n",
    "n_vars = range(8, 19)\n",
    "\n",
    "for i in n_vars:\n",
    "    X_ = X[:,:i]\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='exact-dp') # << BNSL done here!\n",
    "    t1.append(time.time() - tic)\n",
    "    p1.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='exact')\n",
    "    t2.append(time.time() - tic)\n",
    "    p2.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='greedy')\n",
    "    t3.append(time.time() - tic)\n",
    "    p3.append(model.log_probability(X_).sum())\n",
    "\n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X_, algorithm='chow-liu')\n",
    "    t4.append(time.time() - tic)\n",
    "    p4.append(model.log_probability(X_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Time to Learn Structure\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"Time (s)\", fontsize=14)\n",
    "plt.xlabel(\"Variables\", fontsize=14)\n",
    "plt.plot(n_vars, t1, c='c', label=\"Exact Shortest\")\n",
    "plt.plot(n_vars, t2, c='m', label=\"Exact A*\")\n",
    "plt.plot(n_vars, t3, c='g', label=\"Greedy\")\n",
    "plt.plot(n_vars, t4, c='r', label=\"Chow-Liu\")\n",
    "plt.legend(fontsize=14, loc=2)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"$P(D|M)$ with Resulting Model\", fontsize=14)\n",
    "plt.xlabel(\"Variables\", fontsize=14)\n",
    "plt.ylabel(\"logp\", fontsize=14)\n",
    "plt.plot(n_vars, p1, c='c', label=\"Exact Shortest\")\n",
    "plt.plot(n_vars, p2, c='m', label=\"Exact A*\")\n",
    "plt.plot(n_vars, p3, c='g', label=\"Greedy\")\n",
    "plt.plot(n_vars, p4, c='r', label=\"Chow-Liu\")\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the expected results-- that the A\\* algorithm works faster than the\n",
    "shortest path, the greedy one faster than that, and Chow-Liu the fastest. The\n",
    "purple and cyan lines superimpose on the right plot as they produce graphs with\n",
    "the same score, followed closely by the greedy algorithm and then Chow-Liu\n",
    "performing the worst.\n",
    "\n",
    "### Constraint Graphs\n",
    "\n",
    "Now, sometimes you have prior information about how\n",
    "groups of nodes are connected to each other and want to exploit that. This can\n",
    "take the form of a global ordering, where variables can be ordered in such a\n",
    "manner that edges only go from left to right, for example. However, sometimes\n",
    "you have layers in your network where variables are a part of these layers and\n",
    "can only have parents in another layer.\n",
    "\n",
    "Lets consider a diagnostics Bayesian\n",
    "network like the following (no need to read code, the picture is all that is\n",
    "important for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import DiscreteDistribution, ConditionalProbabilityTable, Node\n",
    "\n",
    "BRCA1 = DiscreteDistribution({0: 0.999, 1: 0.001})\n",
    "BRCA2 = DiscreteDistribution({0: 0.985, 1: 0.015})\n",
    "LCT   = DiscreteDistribution({0: 0.950, 1: 0.050})\n",
    "\n",
    "OC = ConditionalProbabilityTable([[0, 0, 0, 0.999],\n",
    "                                  [0, 0, 1, 0.001],\n",
    "                                  [0, 1, 0, 0.750],\n",
    "                                  [0, 1, 1, 0.250],\n",
    "                                  [1, 0, 0, 0.700],\n",
    "                                  [1, 0, 1, 0.300],\n",
    "                                  [1, 1, 0, 0.050],\n",
    "                                  [1, 1, 1, 0.950]], [BRCA1, BRCA2])\n",
    "\n",
    "LI = ConditionalProbabilityTable([[0, 0, 0.99],\n",
    "                                  [0, 1, 0.01],\n",
    "                                  [1, 0, 0.20],\n",
    "                                  [1, 1, 0.80]], [LCT])\n",
    "\n",
    "PREG = DiscreteDistribution({0: 0.90, 1: 0.10})\n",
    "\n",
    "LE = ConditionalProbabilityTable([[0, 0, 0.99],\n",
    "                                  [0, 1, 0.01],\n",
    "                                  [1, 0, 0.25],\n",
    "                                  [1, 1, 0.75]], [OC])\n",
    "\n",
    "BLOAT = ConditionalProbabilityTable([[0, 0, 0, 0.85],\n",
    "                                     [0, 0, 1, 0.15],\n",
    "                                     [0, 1, 0, 0.70],\n",
    "                                     [0, 1, 1, 0.30],\n",
    "                                     [1, 0, 0, 0.40],\n",
    "                                     [1, 0, 1, 0.60],\n",
    "                                     [1, 1, 0, 0.10],\n",
    "                                     [1, 1, 1, 0.90]], [OC, LI])\n",
    "\n",
    "LOA = ConditionalProbabilityTable([[0, 0, 0, 0.99],\n",
    "                                   [0, 0, 1, 0.01],\n",
    "                                   [0, 1, 0, 0.30],\n",
    "                                   [0, 1, 1, 0.70],\n",
    "                                   [1, 0, 0, 0.95],\n",
    "                                   [1, 0, 1, 0.05],\n",
    "                                   [1, 1, 0, 0.95],\n",
    "                                   [1, 1, 1, 0.05]], [PREG, OC])\n",
    "\n",
    "VOM = ConditionalProbabilityTable([[0, 0, 0, 0, 0.99],\n",
    "                                   [0, 0, 0, 1, 0.01],\n",
    "                                   [0, 0, 1, 0, 0.80],\n",
    "                                   [0, 0, 1, 1, 0.20],\n",
    "                                   [0, 1, 0, 0, 0.40],\n",
    "                                   [0, 1, 0, 1, 0.60],\n",
    "                                   [0, 1, 1, 0, 0.30],\n",
    "                                   [0, 1, 1, 1, 0.70],\n",
    "                                   [1, 0, 0, 0, 0.30],\n",
    "                                   [1, 0, 0, 1, 0.70],\n",
    "                                   [1, 0, 1, 0, 0.20],\n",
    "                                   [1, 0, 1, 1, 0.80],\n",
    "                                   [1, 1, 0, 0, 0.05],\n",
    "                                   [1, 1, 0, 1, 0.95],\n",
    "                                   [1, 1, 1, 0, 0.01],\n",
    "                                   [1, 1, 1, 1, 0.99]], [PREG, OC, LI])\n",
    "\n",
    "AC = ConditionalProbabilityTable([[0, 0, 0, 0.95],\n",
    "                                  [0, 0, 1, 0.05],\n",
    "                                  [0, 1, 0, 0.01],\n",
    "                                  [0, 1, 1, 0.99],\n",
    "                                  [1, 0, 0, 0.40],\n",
    "                                  [1, 0, 1, 0.60],\n",
    "                                  [1, 1, 0, 0.20],\n",
    "                                  [1, 1, 1, 0.80]], [PREG, LI])\n",
    "\n",
    "s1 = Node(BRCA1, name=\"BRCA1\")\n",
    "s2 = Node(BRCA2, name=\"BRCA2\")\n",
    "s3 = Node(LCT, name=\"LCT\")\n",
    "s4 = Node(OC, name=\"OC\")\n",
    "s5 = Node(LI, name=\"LI\")\n",
    "s6 = Node(PREG, name=\"PREG\")\n",
    "s7 = Node(LE, name=\"LE\")\n",
    "s8 = Node(BLOAT, name=\"BLOAT\")\n",
    "s9 = Node(LOA, name=\"LOA\")\n",
    "s10 = Node(VOM, name=\"VOM\")\n",
    "s11 = Node(AC, name=\"AC\")\n",
    "\n",
    "model = BayesianNetwork(\"Hut\")\n",
    "model.add_nodes(s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11)\n",
    "model.add_edge(s1, s4)\n",
    "model.add_edge(s2, s4)\n",
    "model.add_edge(s3, s5)\n",
    "model.add_edge(s4, s7)\n",
    "model.add_edge(s4, s8)\n",
    "model.add_edge(s4, s9)\n",
    "model.add_edge(s4, s10)\n",
    "model.add_edge(s5, s8)\n",
    "model.add_edge(s5, s10)\n",
    "model.add_edge(s5, s11)\n",
    "model.add_edge(s6, s9)\n",
    "model.add_edge(s6, s10)\n",
    "model.add_edge(s6, s11)\n",
    "model.bake()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "model.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network contains three layer, with symptoms on the bottom (low energy,\n",
    "bloating, loss of appetite, vomitting, and abdominal cramps), diseases in the\n",
    "middle (overian cancer, lactose intolerance, and pregnancy), and genetic tests\n",
    "on the top for three different genetic mutations. The edges in this graph are\n",
    "constrainted such that symptoms are explained by diseases, and diseases can be\n",
    "partially explained by genetic mutations. There are no edges from diseases to\n",
    "genetic conditions, and no edges from genetic conditions to symptoms. If we were\n",
    "going to design a more efficient search algorithm, we would want to exploit this\n",
    "fact to drastically reduce the search space of graphs.\n",
    "\n",
    "Before presenting a\n",
    "solution, lets also consider another situation. In some cases you can define a\n",
    "global ordering of the variables, meaning you can order them from left to right\n",
    "and ensure that edges only go from the left to the right. This can represent\n",
    "some temporal separation (things on the left happen before things on the right),\n",
    "physical separation, or anything else. This would also dramatically reduce the\n",
    "search space. \n",
    "\n",
    "In addition to reducing the search space, an efficient algorithm\n",
    "can exploit this layered structure. A key property of most scoring functions is\n",
    "the idea of \"global parameter independence\", meaning that that the parents of\n",
    "node A are independent of the parents of node B assuming that they do not form a\n",
    "cycle in the graph. If you have a layered structure, either like in the\n",
    "diagnostics network or through a global ordering, it is impossible to form a\n",
    "cycle in the graph through any valid assignment of parent values. This means\n",
    "that the parents for each node can be identified independently, drastically\n",
    "reducing the runtime of the algorithm.\n",
    "\n",
    "Now, sometimes we know ~some things~\n",
    "about the structure of the variables, but nothing about the others. For example,\n",
    "we might have a partial ordering on some variables but not know anything about\n",
    "the others. We could enforce an arbitrary ordering on the others, but this may\n",
    "not be well justified. In essence, we'd like to exploit whatever information we\n",
    "have.\n",
    "\n",
    "Abstractly, we can think about this in terms of constraint graphs. Lets\n",
    "say you have some symptoms, diseases, and genetic tests, and don't a priori know\n",
    "the connection between all of these pieces, but you do know the previous layer\n",
    "structure. You can define a \"constraint graph\" which is made up of three nodes,\n",
    "\"symptoms\", \"diseases\", and \"genetic mutations\". There is a directed edge from\n",
    "genetic mutations to diseases, and a directed edge from diseases to symptoms.\n",
    "This specifies that genetic mutations can be parents to diseases, and diseases\n",
    "to symptoms. It would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "from pomegranate.utils import plot_networkx\n",
    "\n",
    "constraints = networkx.DiGraph()\n",
    "constraints.add_edge('genetic conditions', 'diseases')\n",
    "constraints.add_edge('diseases', 'symptoms')\n",
    "plot_networkx(constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables corresponding to these categories would be put in their\n",
    "appropriate name. This would define a scaffold for structure learning.\n",
    "\n",
    "Now, we\n",
    "can do the same thing for a global ordering. Lets say we have 3 variables in an\n",
    "order from 0-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = networkx.DiGraph()\n",
    "constraints.add_edge(0, 1)\n",
    "constraints.add_edge(1, 2)\n",
    "constraints.add_edge(0, 2)\n",
    "plot_networkx(constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, we're saying that variable 0 can be a parent for 1 or 2, and that\n",
    "variable 1 can be a parent for variable 2. In the same way that putting multiple\n",
    "variables in a node of the constraint graph allowed us to define layers, putting\n",
    "a single variable in the nodes of a constraint graph can allow us to define an\n",
    "ordering.\n",
    "\n",
    "To be specific, lets say we want to find the parents of the variables\n",
    "in node 1 given that those variables parents can only come from the variables in\n",
    "node 0. We can independently find the best parents for each variable in node 1\n",
    "from the set of those in node 0. This is significantly faster than trying to\n",
    "find the best Bayesian network of all variables in nodes 0 and 1. We can also do\n",
    "the same thing for the variables in node 2 by going through the variables in\n",
    "both nodes 0 and 1 to find the best parent set for the variables in node 2.\n",
    "However, there are some cases where we know nothing about the parent structure\n",
    "of some variables. This can be solved by including self-loops in the graph,\n",
    "where a node is its own parent. This means that we know nothing about the parent\n",
    "structure of the variables in that node and that the full exponential time\n",
    "algorithm will have to be run. The naive structure learning algorithm can be\n",
    "thought of as putting all variables in a single node in the constraint graph and\n",
    "putting a self-loop on that node. \n",
    "\n",
    "We are thus left with two procedures; one\n",
    "for solving edges which are self edges, and one for solving edges which are not.\n",
    "Even though we have to use the exponential time procedure on variables in nodes\n",
    "with self loops, it will still be significantly faster because we will be using\n",
    "less variables (except in the naive case).\n",
    "\n",
    "Frequently though we will have some\n",
    "information about some of the nodes of the graph even if we don't have\n",
    "information about all of the nodes. Lets take the case where we know some\n",
    "variables have no children but can have parents, and know nothing about the\n",
    "other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = networkx.DiGraph()\n",
    "constraints.add_edge(0, 1)\n",
    "constraints.add_edge(0, 0)\n",
    "plot_networkx(constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation we would have to run the exponential time algorithm on the\n",
    "variables in node 0 to find the optimal parents, and then run the independent\n",
    "parents algorithm on the variables in node 1 drawing only from the variables in\n",
    "node 0. To be specific:\n",
    "\n",
    "(1) Use exponential time procedure to find optimal\n",
    "structure amongst variables in node 0 \n",
    "(2) Use independent-parents procedure to\n",
    "find the best parents of variables in node 1, restricting the parents to be in\n",
    "node 0\n",
    "(3) Concatenate these parent sets together to get the optimal structure\n",
    "of the network given the constraints.\n",
    "\n",
    "We can generalize this to any arbitrary\n",
    "constraint graph:\n",
    "\n",
    "(1) Use exponential time procedure to find optimal structure\n",
    "amongst variables in nodes with self loops (including parents from other nodes\n",
    "if needed)\n",
    "(2) Use independent-parents procedure to find best parents of\n",
    "variables in a node given the constraint that the parents must come from\n",
    "variables in the node which is this nodes parent\n",
    "(3) Concatenate these parent\n",
    "sets together to get the optimal structure of the network given the constraints.\n",
    "According to the global parameter independence property of Bayesian networks,\n",
    "this procedure will give the globally optimal Bayesian network while exploring a\n",
    "significantly smaller part of the network.\n",
    "\n",
    "pomegranate supports constraint\n",
    "graphs in an extremely easy to use manner. Lets say that we have a graph with\n",
    "three layers like the diagnostic model, and five variables in each layer. We can\n",
    "define the constraint graph as a networkx DiGraph, with the nodes being tuples\n",
    "containing the column ids of each variable belonging to that variable.\n",
    "\n",
    "In this\n",
    "case, we're saying that (0, 1, 2, 3, 4) is the first node, (5, 6, 7, 8, 9) is\n",
    "the second node, and (10, 11, 12, 13, 14) is the final node. Lets make nodes 1,\n",
    "7, and 12 related, 11, 13, 14 related, and 3 and 5 related. In this case, where\n",
    "should be an edge from 1 to 7, and 7 to 12. 11, 13, and 14 are all a part of the\n",
    "same layer and so that connection should be ignored, and then there should be a\n",
    "connection from 3 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(6)\n",
    "\n",
    "X = numpy.random.randint(2, size=(200, 15))\n",
    "\n",
    "X[:,1] = X[:,7]\n",
    "X[:,12] = 1 - X[:,7]\n",
    "\n",
    "X[:,5] = X[:,3]\n",
    "\n",
    "X[:,13] = X[:,11]\n",
    "X[:,14] = X[:,11]\n",
    "\n",
    "a = networkx.DiGraph()\n",
    "b = tuple((0, 1, 2, 3, 4))\n",
    "c = tuple((5, 6, 7, 8, 9))\n",
    "d = tuple((10, 11, 12, 13, 14))\n",
    "\n",
    "a.add_edge(b, c)\n",
    "a.add_edge(c, d)\n",
    "print( \"Constraint Graph\")\n",
    "plot_networkx(a)\n",
    "plt.show()\n",
    "\n",
    "print( \"Learned Bayesian Network\")\n",
    "\n",
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X, algorithm='exact', constraint_graph=a)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "model.plot()\n",
    "plt.show()\n",
    "print( \"pomegranate time: \", time.time() - tic, model.structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that reconstructed perfectly here. Lets see what would happen if we\n",
    "didn't use the exact algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model = BayesianNetwork.from_samples(X, algorithm='exact')\n",
    "plt.figure(figsize=(16, 8))\n",
    "model.plot()\n",
    "plt.show()\n",
    "print( \"pomegranate time: \", time.time() - tic, model.structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we got three desirable attributes by using a constraint graph. The\n",
    "first is that there was over an order of magnitude speed improvement in finding\n",
    "the optimal graph. The second is that we were able to remove some edges we\n",
    "didn't want in the final Bayesian network, such as those between 11, 13, and 14.\n",
    "We also removed the edge between 1 and 12 and 1 and 3, which are spurious given\n",
    "the model that we originally defined. The third desired attribute is that we can\n",
    "specify the direction of some of the edges and get a better causal model.\n",
    "\n",
    "Lets\n",
    "take a look at how big of a model we can learn given a three layer constraint\n",
    "graph like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_times, times = [], []\n",
    "\n",
    "x = numpy.arange(1, 7)\n",
    "for i in x:\n",
    "    symptoms = tuple(range(i))\n",
    "    diseases = tuple(range(i, i*2))\n",
    "    genetic = tuple(range(i*2, i*3))\n",
    "\n",
    "    constraints = networkx.DiGraph()\n",
    "    constraints.add_edge(genetic, diseases)\n",
    "    constraints.add_edge(diseases, symptoms)\n",
    "    \n",
    "    X = numpy.random.randint(2, size=(2000, i*3))\n",
    "    \n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X, algorithm='exact', constraint_graph=constraints)\n",
    "    constraint_times.append( time.time() - tic )\n",
    "    \n",
    "    tic = time.time()\n",
    "    model = BayesianNetwork.from_samples(X, algorithm='exact')\n",
    "    times.append( time.time() - tic )  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Time To Learn Bayesian Network', fontsize=18)\n",
    "plt.xlabel(\"Number of Variables\", fontsize=14)\n",
    "plt.ylabel(\"Time (s)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.plot( x*3, times, linewidth=3, color='c', label='Exact')\n",
    "plt.plot( x*3, constraint_times, linewidth=3, color='m', label='Constrained')\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like by including a constraint graph, we can far more quickly find the\n",
    "optimal Bayesian network. This can allow us to create Bayesian networks on\n",
    "dozens of variables as long as we can order them into layers.\n",
    "\n",
    "## Other\n",
    "Parameters\n",
    "\n",
    "There are three other parameters that are worth mentioning. The\n",
    "first is that a vector of weights can be passed in alongside a matrix of\n",
    "samples, with each entry in the vector corresponding to the weight of that\n",
    "sample. By default all weights are set to 1, but they can be any non-negative\n",
    "number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianNetwork.from_samples(X, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next\n",
    "is the pseudocounts parameter that indicates the number added to the\n",
    "observations of each observed variable combination. This acts as a regularizer\n",
    "by smoothing over the counts and is typically used to prevent the model from\n",
    "saying it is impossible for a combination of variables to occur together just\n",
    "because they weren't observed in the data. Currently all learning algorithms\n",
    "support pseudocounts. By default this is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =\n",
    "BayesianNetwork.from_samples(X, pseudocount=1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the maximum number\n",
    "of parents that each variable can have can be limited. This is sometimes\n",
    "referred to as the k-learn problem in literature. In practice this can\n",
    "dramatically speed up the BNSL task as the majority of time is spent determining\n",
    "the best parent sets. Currently this only affects the exact and greedy algorithm\n",
    "as in the Chow-Liu algorithm each variable only has a single parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "= BayesianNetwork.from_samples(X, algorithm='exact', max_parents=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "exact inference uses minimum description length as a score function, the maximum\n",
    "number of parents is by default set to $\\log \\left(\\frac{n}{\\log(n)} \\right)$\n",
    "with $n$ being the number of samples in the dataset.\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "pomegranate\n",
    "currently supports exact BNSL through an a shortest path dynamic programming\n",
    "algorithm, an A\\* algorithm, a greedy algorithm, the Chow-Liu tree building\n",
    "algorithm, and a constraint-graph based algorithm which can significantly speed\n",
    "up structure learning if you have any prior knowledge of the interactions\n",
    "between variables.\n",
    "\n",
    "If you have any suggestions for how to improve pomegranate\n",
    "or ideas for algorithms to implement, feel free to open an issue on the issue\n",
    "tracker! I'd love to hear feedback."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
