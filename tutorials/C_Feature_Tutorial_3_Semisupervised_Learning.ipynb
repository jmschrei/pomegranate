{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning\n",
    "\n",
    "author: Jacob Schreiber <br>\n",
    "contact:\n",
    "jmschreiber91@gmail.com\n",
    "\n",
    "Most classical machine learning algorithms either\n",
    "assume that an entire dataset is either labeled (supervised learning) or that\n",
    "there are no labels (unsupervised learning). However, frequently it is the case\n",
    "that some labeled data is present but there is a great deal of unlabeled data as\n",
    "well. A great example of this is that of computer vision where the internet is\n",
    "filled of pictures (mostly of cats) that could be useful, but you don't have the\n",
    "time or money to label them all in accordance with your specific task. Typically\n",
    "what ends up happening is that either the unlabeled data is discarded in favor\n",
    "of training a model solely on the labeled data, or that an unsupervised model is\n",
    "initialized with the labeled data and then set free on the unlabeled data.\n",
    "Neither method uses both sets of data in the optimization process.\n",
    "\n",
    "Semi-\n",
    "supervised learning is a method to incorporate both labeled and unlabeled data\n",
    "into the training task, typically yield better performing estimators than using\n",
    "the labeled data alone. There are many methods one could use for semisupervised\n",
    "learning, and <a href=\"http://scikit-\n",
    "learn.org/stable/modules/label_propagation.html\">scikit-learn has a good write-\n",
    "up on some of these techniques</a>.\n",
    "\n",
    "pomegranate natively implements semi-\n",
    "supervised learning through the a merger of maximum-likelihood and expectation-\n",
    "maximization. As an overview, the models are initialized by first fitting to the\n",
    "labeled data directly using maximum-likelihood estimates. The models are then\n",
    "refined by running expectation-maximization (EM) on the unlabeled datasets and\n",
    "adding the sufficient statistics to those acquired from maximum-likelihood\n",
    "estimates on the labeled data. Under the hood both a supervised model and an\n",
    "unsupervised mixture model are created using the same underlying distribution\n",
    "objects. The summarize method is first called using the supervised method on the\n",
    "labeled data, and then the summarize method is called again using the\n",
    "unsupervised method on the unlabeled data. This causes the sufficient statistics\n",
    "to be updated appropriately given the results of first maximum-likelihood and\n",
    "then EM. This process continues until convergence in the EM step.\n",
    "\n",
    "Let's take a\n",
    "look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pandas\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "import itertools\n",
    "\n",
    "from pomegranate import *\n",
    "\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,scipy,pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pomegranate import *\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn, time\n",
    "seaborn.set_style('whitegrid')\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some data in the form of blobs that are close together.\n",
    "Generally one tends to have far more unlabeled data than labeled data, so let's\n",
    "say that a person only has 50 samples of labeled training data and 4950\n",
    "unlabeled samples. In pomegranate you a sample can be specified as lacking a\n",
    "label by providing the integer -1 as the label, just like in scikit-learn. Let's\n",
    "also say there there is a bit of bias in the labeled samples to inject some\n",
    "noise into the problem, as otherwise Gaussian blobs are trivially modeled with\n",
    "even a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(10000, 2, 3, cluster_std=2)\n",
    "x_min, x_max = X[:,0].min()-2, X[:,0].max()+2\n",
    "y_min, y_max = X[:,1].min()-2, X[:,1].max()+2\n",
    "\n",
    "X_train = X[:5000]\n",
    "y_train = y[:5000]\n",
    "\n",
    "# Set the majority of samples to unlabeled.\n",
    "y_train[numpy.random.choice(5000, size=4950, replace=False)] = -1\n",
    "\n",
    "# Inject noise into the problem\n",
    "X_train[y_train != -1] += 2.5\n",
    "\n",
    "X_test = X[5000:]\n",
    "y_test = y[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data when we plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_train[y_train == -1, 0], X_train[y_train == -1, 1], color='0.6')\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color='c')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color='m')\n",
    "plt.scatter(X_train[y_train == 2, 0], X_train[y_train == 2, 1], color='r')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters of unlabeled data seem clear to us, and it doesn't seem like the\n",
    "labeled data is perfectly faithful to these clusters. This can typically happen\n",
    "in a semisupervised setting as well, as the data that is labeled is sometimes\n",
    "biased either because the labeled data was chosen as it was easy to label, or\n",
    "the data was chosen to be labeled in a biased maner.\n",
    "\n",
    "Now let's try fitting a\n",
    "simple naive Bayes classifier to this data and compare the results when using\n",
    "only the labeled data to when using both the labeled and unlabeled data\n",
    "together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = NaiveBayes.from_samples(NormalDistribution, X_train[y_train != -1], y_train[y_train != -1])\n",
    "print( \"Supervised Learning Accuracy: {}\".format((model_a.predict(X_test) == y_test).mean()))\n",
    "\n",
    "model_b = NaiveBayes.from_samples(NormalDistribution, X_train, y_train)\n",
    "print( \"Semisupervised Learning Accuracy: {}\".format((model_b.predict(X_test) == y_test).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we get a big bump in test set accuracy when we use semi-supervised\n",
    "learning. Let's visualize the data to get a better sense of what is happening\n",
    "here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(X, y, Z):\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], color='0.2', alpha=0.5, s=20)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='c', s=20)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='m', s=20)\n",
    "    plt.scatter(X[y == 2, 0], X[y == 2, 1], color='r', s=20)\n",
    "    plt.contour(xx, yy, Z)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, 0.1), numpy.arange(y_min, y_max, 0.1))\n",
    "Z1 = model_a.predict(numpy.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "Z2 = model_b.predict(numpy.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(221)\n",
    "plt.title(\"Training Data, Supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_train, y_train, Z1)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title(\"Training Data, Semi-supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_train, y_train, Z2)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"Test Data, Supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_test, y_test, Z1)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"Test Data, Semi-supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_test, y_test, Z2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contours plot the decision boundaries between the different classes with the\n",
    "left figures corresponding to the partially labeled training set and the right\n",
    "figures corresponding to the test set. We can see that the boundaries learning\n",
    "using only the labeled data look a bit weird when considering the unlabeled\n",
    "data, particularly in that it doesn't cleanly separate the cyan cluster from the\n",
    "other two. In addition, it seems like the boundary between the magenta and red\n",
    "clusters is a bit curved in an unrealistic way. We would not expect points that\n",
    "fell around (-18, -7) to actually come from the red class. Training the model in\n",
    "a semi-supervised manner cleaned up both of these concerns by learning better\n",
    "boundaries that are also flatter and more generalizable.\n",
    "\n",
    "Let's next compare the\n",
    "training times to see how much slower it is to do semi-supervised learning than\n",
    "it is to do simple supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Supervised Learning: \")\n",
    "%timeit NaiveBayes.from_samples(NormalDistribution, X_train[y_train != -1], y_train[y_train != -1])\n",
    "print( \"Semi-supervised Learning: \")\n",
    "%timeit NaiveBayes.from_samples(NormalDistribution, X_train, y_train)\n",
    "print( \"Label Propagation (sklearn): \")\n",
    "%timeit LabelPropagation().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite a bit slower to do semi-supervised learning than simple supervised\n",
    "learning in this example. This is expected as the simple supervised update for\n",
    "naive Bayes is a trivial MLE across each dimension whereas the semi-supervised\n",
    "case requires EM to converge to complete. However, it is still faster to do\n",
    "semi-supervised learning this setting to learn a naive Bayes classifier than it\n",
    "is to fit the label propagation estimator from sklearn. \n",
    "\n",
    "However, though it is\n",
    "widely used, the naive Bayes classifier is still a fairly simple model. One can\n",
    "construct a more complicated model that does not assume feature independence\n",
    "called a Bayes classifier that can also be trained using semi-supervised\n",
    "learning in pretty much the same manner. You can read more about the Bayes\n",
    "classifier in its tutorial in the tutorial folder. Let's move on to more\n",
    "complicated data and try to fit a mixture model Bayes classifier, comparing the\n",
    "performance between using only labeled data and using all data.\n",
    "\n",
    "First let's\n",
    "generate some more complicated, noisier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.empty(shape=(0, 2))\n",
    "X = numpy.concatenate((X, numpy.random.normal(4, 1, size=(3000, 2)).dot([[-2, 0.5], [2, 0.5]])))\n",
    "X = numpy.concatenate((X, numpy.random.normal(3, 1, size=(6500, 2)).dot([[-1, 2], [1, 0.8]])))\n",
    "X = numpy.concatenate((X, numpy.random.normal(7, 1, size=(8000, 2)).dot([[-0.75, 0.8], [0.9, 1.5]])))\n",
    "X = numpy.concatenate((X, numpy.random.normal(6, 1, size=(2200, 2)).dot([[-1.5, 1.2], [0.6, 1.2]])))\n",
    "X = numpy.concatenate((X, numpy.random.normal(8, 1, size=(3500, 2)).dot([[-0.2, 0.8], [0.7, 0.8]])))\n",
    "X = numpy.concatenate((X, numpy.random.normal(9, 1, size=(6500, 2)).dot([[-0.0, 0.8], [0.5, 1.2]])))\n",
    "x_min, x_max = X[:,0].min()-2, X[:,0].max()+2\n",
    "y_min, y_max = X[:,1].min()-2, X[:,1].max()+2\n",
    "\n",
    "y = numpy.concatenate((numpy.zeros(9500), numpy.ones(10200), numpy.ones(10000)*2))\n",
    "idxs = numpy.arange(29700)\n",
    "numpy.random.shuffle(idxs)\n",
    "\n",
    "X = X[idxs]\n",
    "y = y[idxs]\n",
    "\n",
    "X_train, X_test = X[:25000], X[25000:]\n",
    "y_train, y_test = y[:25000], y[25000:]\n",
    "y_train[numpy.random.choice(25000, size=24920, replace=False)] = -1\n",
    "\n",
    "plt.scatter(X_train[y_train == -1, 0], X_train[y_train == -1, 1], color='0.6', s=1)\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color='c', s=10)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color='m', s=10)\n",
    "plt.scatter(X_train[y_train == 2, 0], X_train[y_train == 2, 1], color='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the accuracies that we get when training a model using\n",
    "just the labeled examples versus all of the examples in a semi-supervised\n",
    "manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 0], max_iterations=1)\n",
    "d2 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 1], max_iterations=1)\n",
    "d3 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 2], max_iterations=1)\n",
    "model_a = BayesClassifier([d1, d2, d3]).fit(X_train[y_train != -1], y_train[y_train != -1])\n",
    "print( \"Supervised Learning Accuracy: {}\".format((model_a.predict(X_test) == y_test).mean()))\n",
    "\n",
    "d1 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 0], max_iterations=1)\n",
    "d2 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 1], max_iterations=1)\n",
    "d3 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 2], max_iterations=1)\n",
    "model_b = BayesClassifier([d1, d2, d3])\n",
    "model_b.fit(X_train, y_train)\n",
    "print( \"Semisupervised Learning Accuracy: {}\".format((model_b.predict(X_test) == y_test).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the semi-supervised method performs better. Let's visualize the\n",
    "landscape in the same manner as before in order to see why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, 0.1), numpy.arange(y_min, y_max, 0.1))\n",
    "Z1 = model_a.predict(numpy.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "Z2 = model_b.predict(numpy.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(221)\n",
    "plt.title(\"Training Data, Supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_train, y_train, Z1)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title(\"Training Data, Semi-supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_train, y_train, Z2)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"Test Data, Supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_test, y_test, Z1)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"Test Data, Semi-supervised Boundaries\", fontsize=16)\n",
    "plot_contour(X_test, y_test, Z2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, one would notice that the decision boundaries when using semi-\n",
    "supervised learning are smoother than those when using only a few samples. This\n",
    "can be explained mostly because having more data can generally lead to smoother\n",
    "decision boundaries as the model does not overfit to spurious examples in the\n",
    "dataset. It appears that the majority of the correctly classified samples come\n",
    "from having a more accurate decision boundary for the magenta samples in the\n",
    "left cluster. When using only the labeled samples many of the magenta samples in\n",
    "this region get classified incorrectly as cyan samples. In contrast, when using\n",
    "all of the data these points are all classified correctly.\n",
    "\n",
    "Lastly, let's take a\n",
    "look at a time comparison in this more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 0], max_iterations=1)\n",
    "d2 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 1], max_iterations=1)\n",
    "d3 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 2], max_iterations=1)\n",
    "model = BayesClassifier([d1, d2, d3])\n",
    "\n",
    "print( \"Supervised Learning: \")\n",
    "%timeit model.fit(X_train[y_train != -1], y_train[y_train != -1])\n",
    "\n",
    "d1 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 0], max_iterations=1)\n",
    "d2 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 1], max_iterations=1)\n",
    "d3 = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X_train[y_train == 2], max_iterations=1)\n",
    "model = BayesClassifier([d1, d2, d3])\n",
    "\n",
    "print( \"Semi-supervised Learning: \")\n",
    "%timeit model.fit(X_train, y_train)\n",
    "\n",
    "print\n",
    "print( \"Label Propagation (sklearn): \")\n",
    "%timeit LabelPropagation().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the difference, while still large, is not as large as in the\n",
    "previous example, being only a ~40x difference instead of a ~1000x difference.\n",
    "This is likely because even without the unlabeled data the supervised model is\n",
    "performing EM to train each of the mixtures that are the components of the Bayes\n",
    "classifier. Again, it is faster to do semi-supervised learning in this manner\n",
    "for generative models than it is to perform LabelPropagation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In the real world (ack) there are frequently situations where only a\n",
    "small fraction of the available data has useful labels. Semi-supervised learning\n",
    "provides a framework for leveraging both the labeled and unlabeled aspects of a\n",
    "dataset to learn a sophisticated estimator. In this case, semi-supervised\n",
    "learning plays well with probabilistic models as normal maximum likelihood\n",
    "estimates can be done on the labeled data and expectation-maximization can be\n",
    "run on the unlabeled data using the same distributions.\n",
    "\n",
    "This notebook has\n",
    "covered how to implement semi-supervised learning in pomegranate using both\n",
    "naive Bayes and a Bayes classifier. All one has to do is set the labels of\n",
    "unlabeled samples to -1 and pomegranate will take care of the rest. This can be\n",
    "particularly useful when encountering complex, noisy, data in the real world\n",
    "that aren't neat Gaussian blobs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
