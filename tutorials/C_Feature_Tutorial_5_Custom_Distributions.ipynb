{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Distributions\n",
    "\n",
    "author: Jacob Schreiber <br>\n",
    "contact:\n",
    "jmschreiber91@gmail.com\n",
    "\n",
    "One of the main implementation details of pomegranate\n",
    "is that it decouples the mathematics behind the various models from the\n",
    "likelihood functions that are used in them. For example, the class probabilities\n",
    "calculated for mixture models and Bayes' classifiers involve multiplying a\n",
    "likelihood function and a prior probability. Typically, this likelihood function\n",
    "is assumed to be Gaussian, but there is no algorithmic requirement that it be\n",
    "any specific probability distribution. This is why pomegranate is much more\n",
    "flexible than other packages in terms of the types of distributions that various\n",
    "models can be built using.\n",
    "\n",
    "However, a major limitation in pomegranate for a\n",
    "long time was that while users were free to use any of the built in\n",
    "distributions to build models, they could not define their own in Python. The\n",
    "reason behind this is the Cython backend, which essentially required that the\n",
    "distributions be implemented in Cython.\n",
    "\n",
    "Fortunatly, a recent patch has allowed\n",
    "users to define their own custom distributions. This means that users can define\n",
    "their own distributions, entirely in Python, and plug them in to existing models\n",
    "without modification. This tutorial will show you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pandas\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "import itertools\n",
    "\n",
    "from pomegranate import *\n",
    "\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,scipy,pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new normal distribution\n",
    "\n",
    "Let's start off by building a custom\n",
    "normal distribution. Our goal is to build a pure Python object that has the\n",
    "exact same functionality as the normal distribution that is currently\n",
    "implemented, to demonstrate that the internals are all working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "class NormalDistribution2():\n",
    "    def __init__(self, mu, std):\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.parameters = (self.mu, self.std)\n",
    "        self.d = 1\n",
    "        self.summaries = numpy.zeros(3)\n",
    "\n",
    "    def log_probability(self, X):\n",
    "        return scipy.stats.norm.logpdf(X, self.mu, self.std)\n",
    "\n",
    "    def summarize(self, X, w=None):\n",
    "        if w is None:\n",
    "            w = numpy.ones(X.shape[0])\n",
    "\n",
    "        X = X.reshape(X.shape[0])\n",
    "        self.summaries[0] += w.sum()\n",
    "        self.summaries[1] += X.dot(w)\n",
    "        self.summaries[2] += (X ** 2.).dot(w)\n",
    "\n",
    "    def from_summaries(self, inertia=0.0):\n",
    "        self.mu = self.summaries[1] / self.summaries[0]\n",
    "        self.std = self.summaries[2] / self.summaries[0] - self.summaries[1] ** 2 / (self.summaries[0] ** 2)\n",
    "        self.std = numpy.sqrt(self.std)\n",
    "        self.parameters = (self.mu, self.std)\n",
    "        self.clear_summaries()\n",
    "\n",
    "    def clear_summaries(self, inertia=0.0):\n",
    "        self.summaries = numpy.zeros(3)\n",
    "\n",
    "    @classmethod\n",
    "    def from_samples(cls, X, weights=None):\n",
    "        d = NormalDistribution2(0, 0)\n",
    "        d.summarize(X, weights)\n",
    "        d.from_summaries()\n",
    "        return d\n",
    "\n",
    "    @classmethod\n",
    "    def blank(cls):\n",
    "        return NormalDistribution2(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom objects have a few requirements.\n",
    "\n",
    "(1) An attribute, `d`, that stores\n",
    "the number of dimensions represented by the distribution. For univariate\n",
    "distributions like this normal distribution, this should just be 1.\n",
    "\n",
    "\n",
    "(2) A\n",
    "method, `log_probability(X)`, that takes in a vector of shape (n_samples,) or a\n",
    "matrix of shape (n_samples, d) if multivariate, and returns the log probability\n",
    "of each sample. In this case we're just using the built-in scipy function for\n",
    "calculating log probabilities under a normal distribution for simplicity.\n",
    "\n",
    "(3) A\n",
    "method, `summarize(X, weights=None)`, that takes in a vector of shape\n",
    "(n_samples,) or a matrix of shape (n_samples, d) if multivariate, calculates the\n",
    "sufficient statistics of that batch, and adds them to the growing sufficient\n",
    "statistics. In the case of a normal distribution, our sufficient statistics are\n",
    "the sum of the weights, the sum of the weighted points, and the sum of the\n",
    "weighted points squared. \n",
    "\n",
    "(4) A method, `from_summaries(inertia=0.0)`, that\n",
    "takes uses the stored sufficient statistics in order to update the model\n",
    "parameters. This should also clear the stored sufficient statistics for the next\n",
    "iteration of training.\n",
    "\n",
    "(5) A method, `clear_summaries()`, that clears the\n",
    "stored sufficient statistics. Here, all we're doing is resetting the three\n",
    "stored summaries to 0.\n",
    "\n",
    "(6) A class method, `from_samples(X, weights=None)` that\n",
    "rreturns a distribution that has been fit to the data. Generally this will\n",
    "initialize a dummy distribution and then overwrite the initial parameters using\n",
    "those derived from the data.\n",
    "\n",
    "(7) A class method, `blank()` or `blank(d)` if\n",
    "multivariate, that creates a dummy distribution with the appropriate number of\n",
    "parameters. Here we return a distribution with a mean and a variance of 0. These\n",
    "dummies generally aren't meant to be used.\n",
    "\n",
    "Let's test this against the built in\n",
    "distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalDistribution(2.532, 8.211).log_probability(102.563), NormalDistribution2(2.532, 8.211).log_probability(102.563)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far. Looks like there might be a small difference at very smalll\n",
    "precisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.normal(0.872, 0.57721, size=100000)\n",
    "\n",
    "print (NormalDistribution.from_samples(X).parameters)\n",
    "print (NormalDistribution2.from_samples(X).parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's try putting this into a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.normal(0, 1, size=(1000, 1))\n",
    "X[::2] += 1\n",
    "\n",
    "model1 = GeneralMixtureModel.from_samples(NormalDistribution, 2, X, max_iterations=5, init='first-k', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GeneralMixtureModel.from_samples(NormalDistribution2, 2, X, max_iterations=5, init='first-k', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.log_probability(X).sum(), model2.log_probability(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like both for fitting a mixture model and performing inference\n",
    "that our new Python distribution is identical to the built-in one. Now, how much\n",
    "slower is it to use the Python object versus the Cython one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.normal(0, 1, size=(300000, 1))\n",
    "X[::2] += 1\n",
    "\n",
    "%timeit GeneralMixtureModel.from_samples(NormalDistribution, 2, X, max_iterations=100, init='first-k')\n",
    "%timeit GeneralMixtureModel.from_samples(NormalDistribution2, 2, X, max_iterations=100, init='first-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it can be a fair bit slower. Another drawback of using a Python\n",
    "distibution is that it may be less efficient to do multi-threaded parallelism,\n",
    "because the Python object requires the GIL. However, many numpy operations will\n",
    "drop the GIL and so can actually be used with multi-threading, but that isn't\n",
    "guaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit GeneralMixtureModel.from_samples(NormalDistribution, 2, X, max_iterations=100, init='first-k', n_jobs=2)\n",
    "%timeit GeneralMixtureModel.from_samples(NormalDistribution2, 2, X, max_iterations=100, init='first-k', n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution acts exactly like any other distribution. If we want to use a\n",
    "different distribution to model different features, we can use a mix of custom\n",
    "and built-in distrbutions with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.normal(0, 1, size=(500, 3))\n",
    "X[::2] += 1\n",
    "X[:,1] = numpy.abs(X[:,1])\n",
    "\n",
    "distributions = [NormalDistribution, ExponentialDistribution, NormalDistribution2]\n",
    "model = GeneralMixtureModel.from_samples(distributions, 2, X, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new distribution, the Student T Distribution\n",
    "\n",
    "There have been a\n",
    "few requests to add in the Student T distribution to pomegranate. This is,\n",
    "essentially, a version of the normal distribution that has a heavy tail to\n",
    "reduce the effect of outliers on the model. Now, instead of waiting for me to\n",
    "find time to add it, you can add it in yourself! Here is an example of what a\n",
    "custom student T distribution might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentTDistribution():\n",
    "    def __init__(self, mu, std, df=1.0):\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.df = df\n",
    "        self.parameters = (self.mu, self.std)\n",
    "        self.d = 1\n",
    "        self.summaries = numpy.zeros(3)\n",
    "\n",
    "    def probability(self, X):\n",
    "        return numpy.exp(self.log_probability(X))\n",
    "        \n",
    "    def log_probability(self, X):\n",
    "        return scipy.stats.t.logpdf(X, self.df, self.mu, self.std)\n",
    "\n",
    "    def summarize(self, X, w=None):\n",
    "        if w is None:\n",
    "            w = numpy.ones(X.shape[0])\n",
    "\n",
    "        X = X.reshape(X.shape[0])\n",
    "        self.summaries[0] += w.sum()\n",
    "        self.summaries[1] += X.dot(w)\n",
    "        self.summaries[2] += (X ** 2.).dot(w)\n",
    "\n",
    "    def from_summaries(self, inertia=0.0):\n",
    "        self.mu = self.summaries[1] / self.summaries[0]\n",
    "        self.std = self.summaries[2] / self.summaries[0] - self.summaries[1] ** 2 / (self.summaries[0] ** 2)\n",
    "        self.std = numpy.sqrt(self.std)\n",
    "        self.parameters = (self.mu, self.std)\n",
    "        self.clear_summaries()\n",
    "\n",
    "    def clear_summaries(self, inertia=0.0):\n",
    "        self.summaries = numpy.zeros(3)\n",
    "\n",
    "    @classmethod\n",
    "    def from_samples(cls, X, weights=None, df=1):\n",
    "        d = StudentTDistribution(0, 0, df)\n",
    "        d.summarize(X, weights)\n",
    "        d.from_summaries()\n",
    "        return d\n",
    "\n",
    "    @classmethod\n",
    "    def blank(cls):\n",
    "        return StudentTDistribution(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between the Student T distribution and the normal\n",
    "distribution is the degree of freedom parameter that has to be set in advance---\n",
    "it is not meant to be learned from the data. The higher this parameter, the more\n",
    "like a normal distribution it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = NormalDistribution(0, 1)\n",
    "dt1 = StudentTDistribution(0, 1, 1)\n",
    "dt3 = StudentTDistribution(0, 1, 3)\n",
    "dt8 = StudentTDistribution(0, 1, 8)\n",
    "\n",
    "x = numpy.arange(-6, 6, 0.1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(x, dn.probability(x), label=\"Normal\")\n",
    "plt.plot(x, dt1.probability(x), label=\"Student T, df=1\")\n",
    "plt.plot(x, dt3.probability(x), label=\"Student T, df=3\")\n",
    "plt.plot(x, dt8.probability(x), label=\"Student T, df=8\")\n",
    "plt.ylabel(\"Probability\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick it in a mixture model now and see what the normal and the Student T\n",
    "versions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.normal(-2, 1, size=(1000, 1))\n",
    "X[::2] += 4\n",
    "\n",
    "modeln = GeneralMixtureModel.from_samples(NormalDistribution, 2, X)\n",
    "modelt = GeneralMixtureModel.from_samples(StudentTDistribution, 2, X)\n",
    "\n",
    "x = numpy.arange(-15, 15, 0.1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(x, modeln.probability(x), label=\"Normal Mixture\")\n",
    "plt.plot(x, modelt.probability(x), label=\"Student T Mixture\")\n",
    "plt.ylabel(\"Probability\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look reasonable. The Student T distributions have a smaller valley\n",
    "separating the peaks because they have heavier tails. Additionally, the peaks\n",
    "are closer together for the Student T distribution because the heavier tails\n",
    "assign more credit from points in one cluster to the distribution modeling the\n",
    "other cluster. This pulls the centers together slightly.\n",
    "\n",
    "### Block Distributions\n",
    "\n",
    "Currently, pomegranate supports distributions where the\n",
    "features are either independent, through `IndependentComponentsDistribution`, or\n",
    "have a Gaussian covariance structure, as in `MultivariateGaussianDistribution`.\n",
    "However, sometimes one would want to model their features in blocks, where the\n",
    "covariance amongst features in the block are accounted for. When all your\n",
    "variables are Gaussian, this is simply having an enforced block structure on\n",
    "your covariance matrix. In the more general case, this can take the form of a\n",
    "Bayesian network. \n",
    "\n",
    "This new custom support allows us to do that easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGaussianDistribution():\n",
    "    def __init__(self, distributions):\n",
    "        self.distributions = distributions\n",
    "        self.d = sum([d.d for d in distributions])\n",
    "        \n",
    "    def log_probability(self, X):\n",
    "        i, log_probability = 0, numpy.zeros(X.shape[0])\n",
    "        for distribution in self.distributions:\n",
    "            log_probability += distribution.log_probability(X[:, i:i+distribution.d].copy())\n",
    "            i += distribution.d\n",
    "        return log_probability\n",
    "    \n",
    "    def summarize(self, X, w=None):\n",
    "        i = 0\n",
    "        for distribution in self.distributions:\n",
    "            distribution.summarize(X[:, i:i+distribution.d].copy(), w)\n",
    "            i += distribution.d\n",
    "    \n",
    "    def from_summaries(self, inertia=0.0):\n",
    "        for distribution in self.distributions:\n",
    "            distribution.from_summaries(inertia)\n",
    "\n",
    "    @classmethod\n",
    "    def from_samples(cls, X, weights=None, ds=[]):\n",
    "        distributions = [MultivariateGaussianDistribution.blank(d) for d in ds]\n",
    "        d = BlockGaussianDistribution(distributions)\n",
    "        d.summarize(X, weights)\n",
    "        d.from_summaries()\n",
    "        return d\n",
    "\n",
    "    @classmethod\n",
    "    def blank(cls, ds):\n",
    "        distributions = [MultivariateGaussianDistribution.blank(d) for d in ds]\n",
    "        return BlockGaussianDistribution(distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, `BlockGaussianDistribution` creates multiple\n",
    "`MultivariateGaussianDistribution` objects, each modeling a different set of\n",
    "features. The probability of an example is then the product of the probability\n",
    "of each set of features under their respective distribution.\n",
    "\n",
    "To evaluate this,\n",
    "let's look at the diabetes data set. This is normally a regression task, so\n",
    "we'll have to binarize the response using the median value. Additionally, the\n",
    "data has been scaled for us already, so we'll have to go ahead an unscale it for\n",
    "the purposes of our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pomegranate import LogNormalDistribution, BernoulliDistribution, NormalDistribution\n",
    "\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "X[:,1] = (X[:,1] - X[:,1].min()) / (X[:,1].max() - X[:,1].min())\n",
    "X[:,0] -= X[:,0].min() - 0.001\n",
    "X[:,2] -= X[:,2].min() - 0.001\n",
    "X[:,3] -= X[:,3].min() - 0.001\n",
    "y = y > numpy.median(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four variables correspond to age, sex, BMI, and average blood\n",
    "pressure. The remaining six features are different blood serum measurements. In\n",
    "this example, let's use univariate distributions to model the first four\n",
    "variables, and a multivariate Gaussian with a full covariance matrix to model\n",
    "the last six.\n",
    "\n",
    "To get a sense for what distributions might be a good fit for the\n",
    "first four features, we can visualize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = 'Age', 'Sex', 'BMI', 'Average Blood Pressure'\n",
    "plt.figure(figsize=(14, 2))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    \n",
    "    x = numpy.arange(0.01, 0.3 if i != 1 else 1, 0.01)\n",
    "    \n",
    "    plt.hist(X_train[:,i], bins=10, normed=True)\n",
    "    plt.plot(x, LogNormalDistribution.from_samples(X_train[:,i]).probability(x), label=\"LogNormal\")\n",
    "    plt.plot(x, NormalDistribution.from_samples(X_train[:,i]).probability(x), label=\"Normal\")\n",
    "    plt.title(names[i], fontsize=12)\n",
    "    \n",
    "plt.legend(loc=(1.05, 0.4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like age and BMI may be better modeled by a normal distribution than a\n",
    "log normal distribution, but that average blood pressure is better modeled with\n",
    "a log normal distribution. Due to the binary nature of sex in this data set, a\n",
    "Bernoulli distribution seems like the right choice.\n",
    "\n",
    "Let's now train a Bayes'\n",
    "classifier using our custom block distribution! To evaluate its performance,\n",
    "let's compare against a model that simply uses a single multivariate Gaussian\n",
    "distribution per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [BlockGaussianDistribution([NormalDistribution.blank(),\n",
    "                                 BernoulliDistribution.blank(),\n",
    "                                 NormalDistribution.blank(),\n",
    "                                 LogNormalDistribution.blank(),\n",
    "                                 MultivariateGaussianDistribution.blank(6)]) for j in range(2)]\n",
    "model = BayesClassifier(ds)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model2 = BayesClassifier.from_samples(MultivariateGaussianDistribution, X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test), model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the custom distribution can outperform the simpler, full\n",
    "covariance Gaussian, in this setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
