{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threaded Parallelism\n",
    "\n",
    "author: Jacob Schreiber <br>\n",
    "contact:\n",
    "jmschreiber91@gmail.com\n",
    "\n",
    "pomegranate supports parallelization through a set of\n",
    "built in functions based off of joblib. All computationally intensive functions\n",
    "in pomegranate are implemented in cython with the global interpreter lock (GIL)\n",
    "released, allowing for multithreading to be used for efficient parallel\n",
    "processing. \n",
    "\n",
    "These functions can all be simply parallelized by passing in\n",
    "`n_jobs=X` to the method calls. This tutorial will demonstrate how to use those\n",
    "calls. First we'll look at a simple multivariate Gaussian mixture model, and\n",
    "compare its performance to sklearn. Then we'll look at a hidden Markov model\n",
    "with Gaussian emissions, and lastly we'll look at a mixture of Gaussian HMMs.\n",
    "These can all utilize the build-in parallelization that pomegranate has.\n",
    "\n",
    "Let's\n",
    "dive right in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "\n",
    "from pomegranate import *\n",
    "\n",
    "numpy.random.seed(0)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,scipy,pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples, n_dim, n_classes, alpha=1):\n",
    "    \"\"\"Create a random dataset with n_samples in each class.\"\"\"\n",
    "    \n",
    "    X = numpy.concatenate([numpy.random.normal(i*alpha, 1, size=(n_samples, n_dim)) for i in range(n_classes)])\n",
    "    y = numpy.concatenate([numpy.zeros(n_samples) + i for i in range(n_classes)])\n",
    "    idx = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(idx)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes\n",
    "\n",
    "Let's start off with a simple model like naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "n, d, k = 50000, 1000, 2\n",
    "X, y = create_dataset(n, d, k)\n",
    "\n",
    "print( \"sklearn GNB\")\n",
    "%timeit GaussianNB().fit(X, y)\n",
    "print \"pomegranate GNB\"\n",
    "%timeit NaiveBayes.from_samples(NormalDistribution, X, y)\n",
    "print\n",
    "print \"pomegranate GNB (2 jobs)\"\n",
    "%timeit NaiveBayes.from_samples(NormalDistribution, X, y, n_jobs=2)\n",
    "print\n",
    "print \"pomegranate GNB (4 jobs)\"\n",
    "%timeit NaiveBayes.from_samples(NormalDistribution, X, y, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like scikit-learn starts off as faster on a single thread, but that\n",
    "adding multiple threads will improve pomegranate's performance, typically past\n",
    "that of scikit-learn on a single thread. The reason this is the case is likely\n",
    "because pomegranate has to create one object per feature per class, whereas\n",
    "scikit-learn will just define the means and variances in a matrix. It's\n",
    "efficient when you know what distribution you're implementing in the naive\n",
    "Bayes, but not as modular.\n",
    "\n",
    "### 2. Bayes' Classifier\n",
    "\n",
    "The next complex model is the Bayes' classifier. This\n",
    "model is similar to the naive Bayes' model, but does not require that the\n",
    "features are independent from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"pomegranate GBC\"\n",
    "%timeit BayesClassifier.from_samples(MultivariateGaussianDistribution, X, y)\n",
    "print\n",
    "print \"pomegranate GBC (2 jobs)\"\n",
    "%timeit BayesClassifier.from_samples(MultivariateGaussianDistribution, X, y, n_jobs=2)\n",
    "print\n",
    "print \"pomegranate GBC (4 jobs)\"\n",
    "%timeit BayesClassifier.from_samples(MultivariateGaussianDistribution, X, y, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a nice spead improvement here as well. There is no model to compare\n",
    "against in scikit-learn because their naive Bayes' model assumes that all\n",
    "features are independent.\n",
    "\n",
    "### 3. General Mixture Models\n",
    "\n",
    "Next, lets take a look at Gaussian mixture\n",
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n, d, k = 1000000, 5, 3\n",
    "X, y = create_dataset(n, d, k)\n",
    "\n",
    "print \"sklearn GMM\"\n",
    "%timeit GaussianMixture(n_components=k, covariance_type='full', max_iter=15, tol=1e-10).fit(X)\n",
    "print \n",
    "print \"pomegranate GMM\"\n",
    "%timeit GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, k, X, max_iterations=15, stop_threshold=1e-10)\n",
    "print\n",
    "print \"pomegranate GMM (2 jobs)\"\n",
    "%timeit GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, k, X, n_jobs=2, max_iterations=15, stop_threshold=1e-10)\n",
    "print\n",
    "print \"pomegranate GMM (4 jobs)\"\n",
    "%timeit GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, k, X, n_jobs=4, max_iterations=15, stop_threshold=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like pomegranate can be faster than sklearn at performing 15 iterations\n",
    "of EM on this data set, and that parallelization can speed things up even\n",
    "further. We can ignore the convergence warnings. Here we're just timing a fixed\n",
    "amount of computational work. Different models might converge after a different\n",
    "number of iterations, and we don't want the pseedup to come just from that.\n",
    "Lets now take a look at the time it takes to make predictions using GMMs. Lets\n",
    "fit the model to a small amount of data, and then predict a larger amount of\n",
    "data drawn from the same underlying distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = create_dataset(1000, 25, 2)\n",
    "\n",
    "a = GaussianMixture(k, n_init=1, max_iter=25).fit(X)\n",
    "b = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, k, X, max_iterations=25)\n",
    "\n",
    "X, _ = create_dataset(1000000, 25, 2)\n",
    "\n",
    "print \"sklearn GMM\"\n",
    "%timeit -n 1 a.predict_proba(X)\n",
    "print\n",
    "print \"pomegranate GMM\"\n",
    "%timeit -n 1 b.predict_proba(X)\n",
    "print\n",
    "print \"pomegranate GMM (4 jobs)\"\n",
    "%timeit -n 1 b.predict_proba(X, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like pomegranate can be slightly slower than sklearn when using a\n",
    "single processor, but that it can be parallelized to get faster performance.\n",
    "\n",
    "To\n",
    "ensure that we're getting the exact same results, only faster, lets subtract the\n",
    "predictions from each other and make sure that the sum is equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (b.predict_proba(X) - b.predict_proba(X, n_jobs=4)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no difference between the two.\n",
    "\n",
    "Lets now make sure that pomegranate and\n",
    "sklearn are learning basically the same thing. Lets fit both models to some 2\n",
    "dimensional 2 component data and make sure that they both extract the underlying\n",
    "clusters by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(1000, 2, 2, alpha=2)\n",
    "\n",
    "a = GaussianMixture(2, n_init=1, max_iter=25).fit(X)\n",
    "b = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, 2, X, max_iterations=25)\n",
    "\n",
    "y1, y2 = a.predict(X), b.predict(X)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"sklearn clusters\", fontsize=14)\n",
    "plt.scatter(X[y1==0, 0], X[y1==0, 1], color='m', edgecolor='m')\n",
    "plt.scatter(X[y1==1, 0], X[y1==1, 1], color='c', edgecolor='c')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"pomegranate clusters\", fontsize=14)\n",
    "plt.scatter(X[y2==0, 0], X[y2==0, 1], color='m', edgecolor='m')\n",
    "plt.scatter(X[y2==1, 0], X[y2==1, 1], color='c', edgecolor='c')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're getting the same basic results for the two. The two\n",
    "algorithms are initialized a bit differently, and so it can be difficult to\n",
    "directly compare the results between them, but it looks like they're getting\n",
    "roughly the same results.\n",
    "\n",
    "### 3. Multivariate Gaussian HMM\n",
    "\n",
    "Now let's move on to training a hidden Markov\n",
    "model with multivariate Gaussian emissions with a diagonal covariance matrix.\n",
    "We'll randomly generate some Gaussian distributed numbers and use pomegranate\n",
    "with either one or four threads to fit our model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.randn(1000, 500, 50)\n",
    "\n",
    "print \"pomegranate Gaussian HMM (1 job)\"\n",
    "%timeit -n 1 -r 1 HiddenMarkovModel.from_samples(NormalDistribution, 5, X, max_iterations=5)\n",
    "print\n",
    "print \"pomegranate Gaussian HMM (2 jobs)\"\n",
    "%timeit -n 1 -r 1 HiddenMarkovModel.from_samples(NormalDistribution, 5, X, max_iterations=5, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we had to do was pass in the n_jobs parameter to the fit function in order\n",
    "to get a speed improvement. It looks like we're getting a really good speed\n",
    "improvement, as well! This is mostly because the HMM algorithms perform a lot\n",
    "more operations than the other models, and so spend the vast majority of time\n",
    "with the GIL released. You may not notice as strong speedups when using a\n",
    "MultivariateGaussianDistribution because BLAS uses multithreaded operations\n",
    "already internally, even when only one job is specified.\n",
    "\n",
    "Now lets look at the\n",
    "prediction function to make sure the we're getting speedups there as well.\n",
    "You'll have to use a wrapper function to parallelize the predictions for a HMM\n",
    "because it returns an annotated sequence rather than a single value like a\n",
    "classic machine learning model might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HiddenMarkovModel.from_samples(NormalDistribution, 5, X, max_iterations=2, verbose=False)\n",
    "\n",
    "print \"pomegranate Gaussian HMM (1 job)\"\n",
    "%timeit predict_proba(model, X)\n",
    "print\n",
    "print \"pomegranate Gaussian HMM (2 jobs)\"\n",
    "%timeit predict_proba(model, X, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we're getting a speedup on that as well, albiet a minor one.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "Hopefully you'll find pomegranate useful in your work!\n",
    "Parallelization should allow you to train complex models faster than before.\n",
    "Keep in mind though that there is an overhead to using parallel processing, and\n",
    "so it's possible that on some smaller examples it does not work as well. In\n",
    "general, the bigger the dataset, the closer to a linear speedup you'll get with\n",
    "pomegranate.\n",
    "\n",
    "If you have any interesting examples of how you've used\n",
    "pomegranate in your work, I'd love to hear about them. In addition I'd like to\n",
    "hear any feedback you may have on features you'd like to see. Please shoot me an\n",
    "email. Good luck!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
